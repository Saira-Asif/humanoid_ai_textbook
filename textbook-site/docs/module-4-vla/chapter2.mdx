---
title: "Chapter 2: LLM Cognitive Planning"
description: "Implementing Large Language Models for cognitive planning and reasoning in humanoid robots"
estimated_time: 6
week: 12
module: "Module 4: Vision-Language-Action Models"
prerequisites:
  - "intro"
  - "module-1-ros2"
  - "module-2-digital-twin"
  - "module-3-isaac"
  - "module-4-vla/index"
  - "module-4-vla/chapter1"
learning_objectives:
  - "Integrate LLMs for cognitive planning and reasoning in humanoid robots with safety validation"
  - "Implement reasoning algorithms for decision-making under uncertainty in humanoid applications"
  - "Create cognitive planning pipelines with multimodal input processing for humanoid robots"
  - "Optimize LLM performance for real-time humanoid applications with computational constraints"
  - "Validate and verify LLM-generated plans for safety and feasibility in humanoid environments"
sidebar_label: "LLM Cognitive Planning"
difficulty: "Advanced"
tags:
  - "llm"
  - "cognitive-planning"
  - "reasoning"
  - "task-decomposition"
  - "humanoid-robotics"
  - "ai-planning"
  - "natural-language-understanding"
  - "safety-validation"
code_examples:
  total: 8
  languages:
    - "python"
    - "bash"
    - "json"
    - "yaml"
    - "javascript"
    - "typescript"
related_chapters:
  - "module-1-ros2/chapter1"
  - "module-2-digital-twin/chapter1"
  - "module-3-isaac/chapter2"
  - "module-4-vla/chapter1"
  - "module-4-vla/chapter3"
appendix_references:
  - "appendix-a"
  - "appendix-b"
  - "appendix-d"
glossary_terms:
  - "llm"
  - "cognitive-architecture"
  - "task-planning"
  - "hierarchical-planning"
  - "semantic-reasoning"
  - "prompt-engineering"
  - "chain-of-thought"
  - "safety-validation"
---

# Chapter 2: LLM Cognitive Planning - Advanced Reasoning for Humanoid Robots

## Introduction to LLM Cognitive Planning for Humanoids

Large Language Models (LLMs) represent a revolutionary advancement in artificial intelligence, offering humanoid robots sophisticated cognitive capabilities for reasoning, planning, and decision-making. Unlike traditional rule-based systems, LLMs provide the flexibility to understand complex natural language commands, reason about the environment, and generate sophisticated action plans that adapt to changing conditions and contexts.

According to research by Brohan et al. (2023), LLMs integrated with robotic systems can achieve remarkable generalization capabilities, enabling robots to perform novel tasks based on natural language descriptions without explicit programming for each specific scenario. For humanoid robots, this cognitive capability is particularly valuable as it allows for natural interaction with humans and adaptation to complex, human-centric environments.

### Key Capabilities for Humanoid Applications

LLM cognitive planning provides several critical capabilities for humanoid robots:

1. **Natural Language Understanding**: Interpret complex, multi-step commands expressed in natural language
2. **Hierarchical Task Decomposition**: Break down complex tasks into executable subtasks
3. **Contextual Reasoning**: Make decisions based on environmental context and robot state
4. **Adaptive Planning**: Modify plans dynamically based on environmental changes
5. **Safety Validation**: Verify plan safety and feasibility before execution
6. **Learning from Interaction**: Improve performance through experience and feedback

### Cognitive Architecture for Humanoid Robots

Humanoid robots require specialized cognitive architectures that integrate LLM reasoning with real-time control systems. The architecture must balance the deliberative reasoning capabilities of LLMs with the reactive requirements of humanoid control systems.

```python
# cognitive_architecture.py
import asyncio
import json
import time
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import threading
import queue

class PlanningLayer(Enum):
    """
    Hierarchical planning layers for humanoid cognitive architecture.
    """
    TASK_LEVEL = "task_level"      # High-level task planning
    BEHAVIOR_LEVEL = "behavior_level"  # Mid-level behavior sequencing
    MOTOR_LEVEL = "motor_level"    # Low-level motor control

@dataclass
class CognitivePlan:
    """
    Structured representation of a cognitive plan for humanoid execution.
    """
    plan_id: str
    original_command: str
    intent: str
    task_list: List[Dict[str, Any]]
    reasoning_trace: List[str]
    confidence_score: float
    execution_context: Dict[str, Any]
    safety_validated: bool
    estimated_completion_time: float

@dataclass
class TaskStep:
    """
    Individual step in a cognitive plan.
    """
    step_id: str
    action_type: str
    parameters: Dict[str, Any]
    preconditions: List[str]
    effects: List[str]
    priority: int
    timeout: float

class HumanoidCognitiveArchitecture:
    """
    Cognitive architecture for humanoid robots integrating LLM reasoning with control systems.
    """

    def __init__(self,
                 llm_provider: str = "openai",
                 model_name: str = "gpt-4-turbo",
                 safety_threshold: float = 0.8):
        self.llm_provider = llm_provider
        self.model_name = model_name
        self.safety_threshold = safety_threshold

        # Initialize LLM client
        if llm_provider == "openai":
            import openai
            self.client = openai.OpenAI()
        elif llm_provider == "anthropic":
            import anthropic
            self.client = anthropic.Anthropic()

        # Planning components
        self.task_planner = HumanoidTaskPlanner()
        self.reasoning_engine = HumanoidReasoningEngine()
        self.safety_validator = HumanoidSafetyValidator()

        # State management
        self.current_plan = None
        self.execution_history = []
        self.context_buffer = []

        # Communication queues
        self.plan_request_queue = queue.Queue()
        self.plan_execution_queue = queue.Queue()

        print(f"Humanoid cognitive architecture initialized with {model_name}")

    def generate_cognitive_plan(self,
                              natural_language_command: str,
                              environment_context: Dict[str, Any],
                              robot_state: Dict[str, Any]) -> Optional[CognitivePlan]:
        """
        Generate a cognitive plan from natural language command using LLM reasoning.
        """
        # Step 1: Parse and understand the command
        parsed_intent = self.reasoning_engine.parse_intent(natural_language_command)

        # Step 2: Analyze environmental context
        context_analysis = self.reasoning_engine.analyze_context(
            environment_context,
            robot_state
        )

        # Step 3: Generate initial plan using LLM
        raw_plan = self._generate_raw_plan(natural_language_command, context_analysis)

        # Step 4: Structure the plan
        structured_plan = self.task_planner.structure_plan(raw_plan, parsed_intent)

        # Step 5: Validate safety and feasibility
        is_safe, safety_issues = self.safety_validator.validate_plan(structured_plan)

        # Step 6: Calculate confidence score
        confidence_score = self._calculate_plan_confidence(
            structured_plan,
            safety_issues
        )

        # Create cognitive plan
        cognitive_plan = CognitivePlan(
            plan_id=f"plan_{int(time.time())}",
            original_command=natural_language_command,
            intent=parsed_intent,
            task_list=structured_plan,
            reasoning_trace=[],  # Would be populated with actual reasoning steps
            confidence_score=confidence_score,
            execution_context={
                'environment': environment_context,
                'robot_state': robot_state,
                'timestamp': time.time()
            },
            safety_validated=is_safe,
            estimated_completion_time=self._estimate_completion_time(structured_plan)
        )

        return cognitive_plan

    def _generate_raw_plan(self, command: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Generate raw plan using LLM with structured prompting.
        """
        prompt = self._construct_planning_prompt(command, context)

        if self.llm_provider == "openai":
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self._get_planning_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # Lower temperature for more consistent planning
                max_tokens=2000
            )

            raw_plan_text = response.choices[0].message.content

        elif self.llm_provider == "anthropic":
            response = self.client.messages.create(
                model=self.model_name,
                max_tokens=2000,
                temperature=0.3,
                system=self._get_planning_system_prompt(),
                messages=[{"role": "user", "content": prompt}]
            )

            raw_plan_text = response.content[0].text

        # Parse the structured response
        return self._parse_llm_response(raw_plan_text)

    def _construct_planning_prompt(self, command: str, context: Dict[str, Any]) -> str:
        """
        Construct structured prompt for cognitive planning.
        """
        return f"""
        You are a cognitive planning system for a humanoid robot. Given the following natural language command and environmental context, create a detailed plan of actions.

        COMMAND: {command}

        ENVIRONMENT CONTEXT:
        - Current location: {context.get('location', 'unknown')}
        - Available objects: {context.get('objects', [])}
        - Obstacles: {context.get('obstacles', [])}
        - Robot capabilities: {context.get('capabilities', [])}
        - Robot current state: {context.get('robot_state', {})}
        - Safety constraints: {context.get('safety_constraints', [])}

        REQUIREMENTS:
        1. Break down the command into specific, executable tasks
        2. Consider environmental constraints and obstacles
        3. Account for robot capabilities and limitations
        4. Include safety checks between major actions
        5. Provide estimated time for each task
        6. Consider humanoid-specific constraints (balance, bipedal locomotion, anthropomorphic manipulation)

        Respond in JSON format with the following structure:
        {{
            "intent": "brief description of the overall goal",
            "tasks": [
                {{
                    "id": "unique_task_id",
                    "action": "specific action type",
                    "parameters": {{"param_name": "value"}},
                    "preconditions": ["condition1", "condition2"],
                    "effects": ["effect1", "effect2"],
                    "estimated_duration": 5.0,
                    "priority": 1
                }}
            ],
            "reasoning": "Explain the thought process behind the plan"
        }}

        Ensure all actions are safe, feasible, and appropriate for a humanoid robot.
        """

    def _get_planning_system_prompt(self) -> str:
        """
        Get system prompt for planning-specific behavior.
        """
        return """
        You are an expert cognitive planning system for humanoid robots. Your role is to generate detailed, safe, and executable plans from natural language commands. Consider the following:

        1. HUMANOID CONSTRAINTS: Account for balance, joint limits, and bipedal locomotion
        2. SAFETY FIRST: Prioritize safety in all action recommendations
        3. FEASIBILITY: Ensure all actions are physically possible for the robot
        4. EFFICIENCY: Optimize for task completion time while maintaining safety
        5. CLARITY: Provide clear, unambiguous action specifications
        6. MODULARITY: Break complex tasks into simple, executable steps
        7. BALANCE CONSIDERATIONS: Humanoid robots must maintain balance during all actions

        Always respond in the requested JSON format. If uncertain about feasibility, indicate potential risks.
        """

    def _parse_llm_response(self, response_text: str) -> List[Dict[str, Any]]:
        """
        Parse LLM response into structured task list.
        """
        try:
            # Clean up the response to extract JSON
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1

            if json_start != -1 and json_end != 0:
                json_str = response_text[json_start:json_end]
                parsed_response = json.loads(json_str)

                # Extract tasks from the response
                tasks = parsed_response.get('tasks', [])

                # Validate task structure
                validated_tasks = []
                for task in tasks:
                    if self._validate_task_structure(task):
                        validated_tasks.append(task)

                return validated_tasks
            else:
                print(f"Could not parse JSON from LLM response: {response_text}")
                return []

        except json.JSONDecodeError as e:
            print(f"Error parsing LLM response as JSON: {e}")
            return []
        except Exception as e:
            print(f"Unexpected error parsing LLM response: {e}")
            return []

    def _validate_task_structure(self, task: Dict[str, Any]) -> bool:
        """
        Validate that a task has the required structure.
        """
        required_fields = ['action', 'parameters', 'estimated_duration']
        return all(field in task for field in required_fields)

    def _calculate_plan_confidence(self, plan: List[Dict], safety_issues: List[str]) -> float:
        """
        Calculate overall plan confidence based on various factors.
        """
        # Start with base confidence
        confidence = 0.8

        # Reduce confidence for safety issues
        confidence -= len(safety_issues) * 0.1

        # Consider plan complexity
        complexity_factor = min(len(plan) * 0.02, 0.2)  # Up to 20% reduction for complex plans
        confidence -= complexity_factor

        # Ensure confidence stays within bounds
        return max(0.0, min(1.0, confidence))

    def _estimate_completion_time(self, plan: List[Dict[str, Any]]) -> float:
        """
        Estimate total completion time for the plan.
        """
        total_time = sum(task.get('estimated_duration', 1.0) for task in plan)

        # Add overhead for transitions and safety checks
        overhead = len(plan) * 0.5  # 0.5 seconds per transition

        return total_time + overhead

    def execute_plan(self, plan: CognitivePlan) -> bool:
        """
        Execute a cognitive plan on the humanoid robot.
        """
        if not plan.safety_validated:
            print("Plan not safety validated - refusing to execute")
            return False

        if plan.confidence_score < self.safety_threshold:
            print(f"Plan confidence {plan.confidence_score} below threshold {self.safety_threshold}")
            return False

        print(f"Executing plan: {plan.intent}")

        success = True
        for i, task in enumerate(plan.task_list):
            print(f"Executing task {i+1}/{len(plan.task_list)}: {task['action']}")

            task_success = self._execute_task(task, plan.execution_context)
            if not task_success:
                print(f"Task failed: {task['action']}")
                success = False
                break

        # Store execution result
        execution_result = {
            'plan_id': plan.plan_id,
            'success': success,
            'executed_tasks': len([t for t in plan.task_list[:i+1]]),
            'total_tasks': len(plan.task_list),
            'timestamp': time.time()
        }

        self.execution_history.append(execution_result)

        return success

    def _execute_task(self, task: Dict[str, Any], context: Dict[str, Any]) -> bool:
        """
        Execute a single task in the plan.
        """
        # In a real implementation, this would interface with ROS 2
        # For this example, we'll simulate task execution
        action_type = task['action']
        parameters = task['parameters']

        print(f"  - Action: {action_type}")
        print(f"  - Parameters: {parameters}")

        # Simulate execution time
        estimated_duration = task.get('estimated_duration', 1.0)
        time.sleep(min(estimated_duration, 0.1))  # Simulate but don't actually wait long

        # Return success (in real system, would check actual execution)
        return True

def main():
    """
    Example usage of humanoid cognitive architecture.
    """
    cognitive_arch = HumanoidCognitiveArchitecture(
        llm_provider="openai",  # or "anthropic"
        model_name="gpt-4-turbo"
    )

    # Example command
    command = "Navigate to the kitchen, pick up the red cup from the counter, and bring it to me in the living room"

    # Example context (in real system, this would come from perception systems)
    context = {
        'location': 'living room',
        'objects': ['red cup on counter', 'table', 'chair'],
        'obstacles': ['coffee table', 'plant'],
        'capabilities': ['navigation', 'manipulation', 'object recognition'],
        'robot_state': {'battery': 0.85, 'location': 'living_room', 'gripper': 'open'},
        'safety_constraints': ['avoid_humans', 'maintain_balance']
    }

    print(f"Generating plan for: {command}")

    plan = cognitive_arch.generate_cognitive_plan(command, context, context['robot_state'])

    if plan:
        print(f"Generated plan with confidence: {plan.confidence_score:.2f}")
        print(f"Plan has {len(plan.task_list)} tasks")
        print(f"Estimated completion time: {plan.estimated_completion_time:.1f} seconds")

        # Execute the plan
        success = cognitive_arch.execute_plan(plan)
        print(f"Plan execution {'succeeded' if success else 'failed'}")
    else:
        print("Failed to generate plan")

if __name__ == "__main__":
    main()
```

## LLM Integration for Cognitive Planning

### Prompt Engineering for Robotics Applications

Effective LLM integration requires careful prompt engineering tailored to robotic applications:

```python
# prompt_engineering.py
import json
from typing import Dict, List, Any
import re

class HumanoidPromptEngineer:
    """
    Specialized prompt engineering for humanoid robot cognitive planning.
    """

    def __init__(self):
        self.prompt_templates = self._initialize_prompt_templates()
        self.context_enhancers = self._initialize_context_enhancers()

    def _initialize_prompt_templates(self) -> Dict[str, str]:
        """
        Initialize common prompt templates for different types of robotic tasks.
        """
        return {
            'navigation': """
            Plan a navigation task for a humanoid robot. Consider:
            1. Path planning around obstacles
            2. Balance and stability requirements for bipedal locomotion
            3. Safe speed for humanoid walking
            4. Environmental hazards (stairs, narrow passages, etc.)

            COMMAND: {command}
            CURRENT POSITION: {current_position}
            TARGET POSITION: {target_position}
            ENVIRONMENT MAP: {environment_map}
            SAFETY CONSTRAINTS: {safety_constraints}

            Provide step-by-step navigation plan with safety checks.
            """,

            'manipulation': """
            Plan a manipulation task for a humanoid robot. Consider:
            1. Reachability and kinematic constraints
            2. Grasp planning for object manipulation
            3. Balance maintenance during manipulation
            4. Collision avoidance with environment

            COMMAND: {command}
            OBJECT DETAILS: {object_details}
            ROBOT STATE: {robot_state}
            ENVIRONMENT: {environment}
            SAFETY CONSTRAINTS: {safety_constraints}

            Provide detailed manipulation sequence with grasp planning and balance considerations.
            """,

            'interaction': """
            Plan a social interaction for a humanoid robot. Consider:
            1. Appropriate social distance and gaze direction
            2. Natural gesture and posture
            3. Context-appropriate responses
            4. Safety in human-robot proximity

            COMMAND: {command}
            HUMAN CONTEXT: {human_context}
            SOCIAL SETTINGS: {social_settings}
            SAFETY CONSTRAINTS: {safety_constraints}

            Provide interaction plan with appropriate social behaviors.
            """,

            'composite_task': """
            Plan a composite task involving multiple robot capabilities. Consider:
            1. Task decomposition into sequential steps
            2. Resource allocation (arms, mobility, sensors)
            3. Safety transitions between task phases
            4. Error recovery and contingency planning

            COMMAND: {command}
            ROBOT CAPABILITIES: {capabilities}
            ENVIRONMENT: {environment}
            CONSTRAINTS: {constraints}
            SAFETY REQUIREMENTS: {safety_requirements}

            Provide comprehensive plan with task decomposition and safety considerations.
            """
        }

    def _initialize_context_enhancers(self) -> Dict[str, callable]:
        """
        Initialize functions to enhance context for better LLM understanding.
        """
        return {
            'spatial_context': self._enhance_spatial_context,
            'temporal_context': self._enhance_temporal_context,
            'capability_context': self._enhance_capability_context,
            'safety_context': self._enhance_safety_context
        }

    def construct_planning_prompt(self,
                                task_type: str,
                                command: str,
                                context: Dict[str, Any]) -> str:
        """
        Construct a complete planning prompt with enhanced context.
        """
        # Select appropriate template
        template = self.prompt_templates.get(task_type, self.prompt_templates['composite_task'])

        # Enhance context with domain-specific information
        enhanced_context = self._enhance_context(context)

        # Format the template with context
        formatted_prompt = template.format(
            command=command,
            **enhanced_context
        )

        return formatted_prompt

    def _enhance_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance context with domain-specific information.
        """
        enhanced = context.copy()

        # Apply all context enhancers
        for enhancer_name, enhancer_func in self.context_enhancers.items():
            try:
                enhanced = enhancer_func(enhanced)
            except Exception as e:
                print(f"Error in context enhancer {enhancer_name}: {e}")

        return enhanced

    def _enhance_spatial_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance spatial reasoning context for humanoid navigation.
        """
        if 'environment_map' in context:
            # Add spatial reasoning hints
            context['spatial_hints'] = {
                'preferred_paths': ['wide_corridors', 'flat_surfaces', 'well_lit_areas'],
                'avoid_zones': ['stairs', 'narrow_passages', 'cluttered_areas'],
                'waypoints': self._identify_key_waypoints(context['environment_map'])
            }

        return context

    def _enhance_temporal_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance temporal context for task planning.
        """
        # Add time-sensitive information
        context['temporal_constraints'] = {
            'urgency_level': self._determine_urgency(context.get('command', '')),
            'time_windows': context.get('time_windows', []),
            'deadline': context.get('deadline')
        }

        return context

    def _enhance_capability_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance context with robot capability information.
        """
        # Add detailed capability information
        context['detailed_capabilities'] = {
            'locomotion': {
                'max_speed': 0.5,  # m/s
                'turn_rate': 0.5,  # rad/s
                'step_height': 0.15,  # m
                'slope_tolerance': 0.3  # rad
            },
            'manipulation': {
                'reach_volume': 'hemisphere_radius_0.8m',
                'payload_capacity': 2.0,  # kg
                'precision_grip': True,
                'power_grip': True
            },
            'perception': {
                'camera_range': 10.0,  # m
                'lidar_range': 20.0,  # m
                'object_recognition': ['known_objects', 'custom_objects']
            }
        }

        return context

    def _enhance_safety_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance context with safety considerations.
        """
        context['safety_constraints'] = {
            'personal_space': 1.0,  # meter minimum distance from humans
            'collision_avoidance': True,
            'fall_prevention': True,
            'emergency_stop': True,
            'force_limits': {
                'manipulation_force': 50.0,  # N
                'contact_force': 100.0  # N
            }
        }

        return context

    def _identify_key_waypoints(self, environment_map: str) -> List[str]:
        """
        Identify key waypoints from environment description.
        """
        # In a real implementation, this would parse a detailed map
        # For now, return common waypoints in indoor environments
        return ['entrance', 'kitchen', 'living_room', 'bedroom', 'bathroom', 'office']

    def _determine_urgency(self, command: str) -> str:
        """
        Determine urgency level from command text.
        """
        high_urgency_keywords = ['emergency', 'urgent', 'immediately', 'now', 'help', 'danger']
        medium_urgency_keywords = ['soon', 'quickly', 'fast', 'hurry', 'asap']

        command_lower = command.lower()

        for keyword in high_urgency_keywords:
            if keyword in command_lower:
                return 'high'

        for keyword in medium_urgency_keywords:
            if keyword in command_lower:
                return 'medium'

        return 'low'

    def create_chain_of_thought_prompt(self, command: str, context: Dict[str, Any]) -> str:
        """
        Create a chain-of-thought prompt for better reasoning.
        """
        cot_template = """
        Think step-by-step to plan the following humanoid robot task:

        COMMAND: {command}

        CONTEXT: {context}

        STEP-BY-STEP REASONING:
        1. Goal Analysis: What is the ultimate objective?
        2. Current State: What is the robot's current situation?
        3. Constraints: What limitations must be considered?
        4. Subtasks: What are the necessary intermediate steps?
        5. Resources: What capabilities are needed for each step?
        6. Safety: What safety checks are required?
        7. Validation: How will success be confirmed?

        PLAN:
        [Provide structured plan based on the reasoning above]

        Remember to consider humanoid-specific constraints like balance, bipedal locomotion, and anthropomorphic manipulation.
        """

        return cot_template.format(
            command=command,
            context=json.dumps(context, indent=2)
        )

    def create_reflection_prompt(self,
                               initial_plan: str,
                               command: str,
                               context: Dict[str, Any]) -> str:
        """
        Create reflection prompt to validate and improve initial plan.
        """
        reflection_template = """
        Review and validate the following plan for a humanoid robot:

        ORIGINAL COMMAND: {command}

        INITIAL PLAN: {initial_plan}

        CONTEXT: {context}

        VALIDATION CHECKS:
        1. Is the plan safe for both robot and humans?
        2. Are all actions feasible given robot capabilities?
        3. Does the plan consider humanoid-specific constraints?
        4. Are there potential failure modes or risks?
        5. Is the plan efficient and logical?

        IMPROVED PLAN:
        [If needed, provide an improved version of the plan with corrections]

        CONFIDENCE ASSESSMENT:
        [Rate confidence in the plan from 0-10 and explain reasons]
        """

        return reflection_template.format(
            command=command,
            initial_plan=initial_plan,
            context=json.dumps(context, indent=2)
        )

def main():
    """
    Example usage of prompt engineering system.
    """
    prompt_engineer = HumanoidPromptEngineer()

    # Example command
    command = "Go to the kitchen and bring me a glass of water"

    # Example context
    context = {
        'current_position': 'living_room',
        'target_position': 'kitchen',
        'environment_map': 'house_layout_with_rooms_and_obstacles',
        'robot_state': {'battery': 0.8, 'location': 'living_room', 'gripper': 'empty'},
        'capabilities': ['navigation', 'manipulation', 'object_recognition'],
        'safety_constraints': ['avoid_humans', 'maintain_balance']
    }

    # Create planning prompt
    planning_prompt = prompt_engineer.construct_planning_prompt(
        'composite_task',  # This involves both navigation and manipulation
        command,
        context
    )

    print("Constructed Planning Prompt:")
    print("=" * 50)
    print(planning_prompt)
    print("=" * 50)

    # Create chain of thought prompt
    cot_prompt = prompt_engineer.create_chain_of_thought_prompt(command, context)

    print("\nChain of Thought Prompt:")
    print("=" * 50)
    print(cot_prompt)
    print("=" * 50)

if __name__ == "__main__":
    main()
```

### LLM Integration with ROS 2

Connecting LLM cognitive planning with ROS 2 for humanoid robot control:

```python
# llm_ros_integration.py
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Float64
from geometry_msgs.msg import Pose, Twist
from humanoid_interfaces.msg import CognitivePlan, RobotAction, TaskSequence
from humanoid_interfaces.srv import PlanGeneration, PlanExecution
import asyncio
import json
import threading
import queue
from typing import Optional, Dict, Any
import time

class LLMTacticalPlannerNode(Node):
    """
    ROS 2 node for LLM-based tactical planning in humanoid robots.
    """

    def __init__(self):
        super().__init__('llm_tactical_planner_node')

        # Declare parameters
        self.declare_parameter('llm_model', 'gpt-4-turbo')
        self.declare_parameter('llm_temperature', 0.3)
        self.declare_parameter('max_plan_length', 50)
        self.declare_parameter('safety_threshold', 0.75)
        self.declare_parameter('planning_timeout', 30.0)

        # Get parameters
        self.llm_model = self.get_parameter('llm_model').value
        self.temperature = self.get_parameter('llm_temperature').value
        self.max_plan_length = self.get_parameter('max_plan_length').value
        self.safety_threshold = self.get_parameter('safety_threshold').value
        self.planning_timeout = self.get_parameter('planning_timeout').value

        # Initialize LLM client
        self.llm_client = self._initialize_llm_client()

        # Initialize cognitive architecture
        self.cognitive_architecture = HumanoidCognitiveArchitecture(
            llm_provider='openai',  # Could be parameterized
            model_name=self.llm_model,
            safety_threshold=self.safety_threshold
        )

        # Create subscribers
        self.command_subscriber = self.create_subscription(
            String,
            '/voice_command/processed',
            self.command_callback,
            10
        )

        self.environment_subscriber = self.create_subscription(
            String,
            '/environment/context',
            self.environment_callback,
            10
        )

        # Create publishers
        self.plan_publisher = self.create_publisher(
            CognitivePlan,
            '/cognitive_plan',
            10
        )

        self.action_publisher = self.create_publisher(
            RobotAction,
            '/robot/action_command',
            10
        )

        self.status_publisher = self.create_publisher(
            String,
            '/llm_planner/status',
            10
        )

        # Create services
        self.plan_generation_service = self.create_service(
            PlanGeneration,
            '/llm_plan/generate',
            self.generate_plan_callback
        )

        self.plan_execution_service = self.create_service(
            PlanExecution,
            '/llm_plan/execute',
            self.execute_plan_callback
        )

        # Initialize state
        self.current_environment_context = {}
        self.pending_commands = queue.Queue()
        self.active_plans = {}

        # Create timer for processing commands
        self.command_processing_timer = self.create_timer(
            0.5,  # Process commands at 2Hz
            self.process_pending_commands
        )

        self.get_logger().info(f'LLM Tactical Planner initialized with {self.llm_model}')

    def _initialize_llm_client(self):
        """
        Initialize LLM client based on configuration.
        """
        import openai
        return openai.OpenAI(api_key=self.get_llm_api_key())

    def get_llm_api_key(self) -> str:
        """
        Get LLM API key from environment or parameters.
        """
        import os
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            # In a real system, this would be more securely managed
            raise ValueError("OPENAI_API_KEY environment variable not set")
        return api_key

    def command_callback(self, msg: String):
        """
        Handle incoming voice commands for cognitive planning.
        """
        command_text = msg.data
        self.get_logger().info(f'Received command for planning: "{command_text}"')

        # Add to processing queue
        try:
            self.pending_commands.put_nowait({
                'command': command_text,
                'timestamp': self.get_clock().now().to_msg(),
                'source': 'voice'
            })
        except queue.Full:
            self.get_logger().error('Command queue is full, dropping command')

    def environment_callback(self, msg: String):
        """
        Handle incoming environmental context updates.
        """
        try:
            context_data = json.loads(msg.data)
            self.current_environment_context.update(context_data)
            self.get_logger().debug(f'Updated environment context: {list(context_data.keys())}')
        except json.JSONDecodeError as e:
            self.get_logger().error(f'Error parsing environment context: {e}')

    def process_pending_commands(self):
        """
        Process commands from the pending queue.
        """
        try:
            while not self.pending_commands.empty():
                command_item = self.pending_commands.get_nowait()

                # Generate plan for the command
                plan = self.cognitive_architecture.generate_cognitive_plan(
                    command_item['command'],
                    self.current_environment_context,
                    {}  # Would get robot state in real implementation
                )

                if plan and plan.safety_validated and plan.confidence_score >= self.safety_threshold:
                    # Publish the plan
                    self.publish_cognitive_plan(plan)

                    # Store for potential execution
                    self.active_plans[plan.plan_id] = plan

                    self.get_logger().info(f'Published cognitive plan: {plan.plan_id}')
                else:
                    self.get_logger().warn(f'Plan generation failed or safety validation failed for command: {command_item["command"]}')

        except queue.Empty:
            # No commands to process
            pass
        except Exception as e:
            self.get_logger().error(f'Error processing commands: {e}')

    def publish_cognitive_plan(self, plan: CognitivePlan):
        """
        Publish cognitive plan as ROS 2 message.
        """
        plan_msg = CognitivePlan()
        plan_msg.header.stamp = self.get_clock().now().to_msg()
        plan_msg.header.frame_id = 'map'

        plan_msg.plan_id = plan.plan_id
        plan_msg.original_command = plan.original_command
        plan_msg.intent = plan.intent
        plan_msg.confidence_score = plan.confidence_score
        plan_msg.safety_validated = plan.safety_validated
        plan_msg.estimated_completion_time = plan.estimated_completion_time

        # Convert task list to ROS message format
        for task in plan.task_list:
            task_msg = RobotAction()
            task_msg.action_type = task['action']

            # Convert parameters to key-value pairs
            for param_name, param_value in task['parameters'].items():
                task_msg.parameters[param_name] = str(param_value)

            task_msg.estimated_duration = float(task.get('estimated_duration', 1.0))
            task_msg.priority = int(task.get('priority', 1))

            plan_msg.tasks.append(task_msg)

        self.plan_publisher.publish(plan_msg)

    def generate_plan_callback(self, request, response):
        """
        Service callback for plan generation.
        """
        self.get_logger().info(f'Generating plan for command: {request.command}')

        try:
            plan = self.cognitive_architecture.generate_cognitive_plan(
                request.command,
                request.context,
                request.robot_state
            )

            if plan:
                response.success = True
                response.plan_id = plan.plan_id
                response.confidence_score = plan.confidence_score
                response.safety_validated = plan.safety_validated
                response.estimated_completion_time = plan.estimated_completion_time

                # Store plan for execution
                self.active_plans[plan.plan_id] = plan

                self.get_logger().info(f'Plan generated successfully: {plan.plan_id}')
            else:
                response.success = False
                response.error_message = 'Failed to generate plan'

        except Exception as e:
            response.success = False
            response.error_message = f'Error generating plan: {str(e)}'
            self.get_logger().error(response.error_message)

        return response

    def execute_plan_callback(self, request, response):
        """
        Service callback for plan execution.
        """
        plan_id = request.plan_id
        self.get_logger().info(f'Executing plan: {plan_id}')

        if plan_id not in self.active_plans:
            response.success = False
            response.error_message = f'Plan not found: {plan_id}'
            return response

        plan = self.active_plans[plan_id]

        try:
            execution_success = self.cognitive_architecture.execute_plan(plan)

            response.success = execution_success
            response.error_message = '' if execution_success else 'Plan execution failed'

            if execution_success:
                self.get_logger().info(f'Plan executed successfully: {plan_id}')
            else:
                self.get_logger().error(f'Plan execution failed: {plan_id}')

        except Exception as e:
            response.success = False
            response.error_message = f'Error executing plan: {str(e)}'
            self.get_logger().error(response.error_message)

        return response

    def publish_status(self):
        """
        Publish status information.
        """
        status_msg = String()
        status_msg.data = f'LLM Planner active, {len(self.active_plans)} active plans, ' \
                         f'queue size: {self.pending_commands.qsize()}'
        self.status_publisher.publish(status_msg)

def main(args=None):
    rclpy.init(args=args)

    llm_planner_node = LLMTacticalPlannerNode()

    try:
        rclpy.spin(llm_planner_node)
    except KeyboardInterrupt:
        llm_planner_node.get_logger().info('Shutting down LLM Tactical Planner')
    finally:
        llm_planner_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Hierarchical Task Planning for Humanoids

### Multi-Level Planning Architecture

Humanoid robots require hierarchical planning to manage complex behaviors across multiple time scales:

```python
# hierarchical_planner.py
import time
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import networkx as nx
from collections import defaultdict, deque

class TaskPriority(Enum):
    """
    Priority levels for humanoid tasks.
    """
    EMERGENCY = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4

@dataclass
class HumanoidTask:
    """
    Representation of a task in the humanoid planning system.
    """
    task_id: str
    task_type: str
    description: str
    priority: TaskPriority
    preconditions: List[str]
    effects: List[str]
    duration_estimate: float
    dependencies: List[str]
    resources_required: List[str]
    success_criteria: List[str]

class HierarchicalTaskPlanner:
    """
    Hierarchical task planning system for humanoid robots.
    """

    def __init__(self):
        # Task graphs for different planning levels
        self.task_graphs = {
            'strategic': nx.DiGraph(),  # Long-term goals and objectives
            'tactical': nx.DiGraph(),   # Mid-term mission planning
            'operational': nx.DiGraph() # Short-term action planning
        }

        # Task execution queues
        self.execution_queues = {
            level: deque() for level in ['strategic', 'tactical', 'operational']
        }

        # Resource management
        self.resources = {
            'arms': 2,
            'legs': 2,
            'head': 1,
            'navigation_system': 1,
            'manipulation_system': 1,
            'perception_system': 1
        }

        # Task registry
        self.task_registry = {}

        print("Hierarchical task planner initialized")

    def register_task_template(self, task_type: str, template: Dict[str, Any]):
        """
        Register a task template for reuse.
        """
        self.task_registry[task_type] = template
        print(f"Registered task template: {task_type}")

    def create_strategic_plan(self, goals: List[str], time_horizon: float) -> List[HumanoidTask]:
        """
        Create strategic-level plan for long-term objectives.
        """
        strategic_tasks = []

        for i, goal in enumerate(goals):
            task = HumanoidTask(
                task_id=f"strategic_{i}_{int(time.time())}",
                task_type="strategic_goal",
                description=goal,
                priority=TaskPriority.NORMAL,
                preconditions=[],
                effects=[f"goal_{i}_achieved"],
                duration_estimate=time_horizon / len(goals),
                dependencies=[],
                resources_required=["planning_system"],
                success_criteria=[f"goal_{i}_verification"]
            )
            strategic_tasks.append(task)

        # Build strategic task graph
        self._build_task_graph('strategic', strategic_tasks)

        return strategic_tasks

    def decompose_tactical_plan(self, strategic_plan: List[HumanoidTask]) -> List[HumanoidTask]:
        """
        Decompose strategic plan into tactical tasks.
        """
        tactical_tasks = []

        for strategic_task in strategic_plan:
            # Decompose strategic task into tactical subtasks
            subtasks = self._decompose_strategic_task(strategic_task)

            for subtask in subtasks:
                subtask.dependencies.append(strategic_task.task_id)
                tactical_tasks.append(subtask)

        # Build tactical task graph
        self._build_task_graph('tactical', tactical_tasks)

        return tactical_tasks

    def _decompose_strategic_task(self, strategic_task: HumanoidTask) -> List[HumanoidTask]:
        """
        Decompose a strategic task into tactical subtasks.
        """
        subtasks = []

        if "navigation" in strategic_task.description.lower():
            # Navigation task decomposition
            subtasks.extend([
                HumanoidTask(
                    task_id=f"nav_plan_{strategic_task.task_id}",
                    task_type="navigation_planning",
                    description=f"Plan route for {strategic_task.description}",
                    priority=TaskPriority.NORMAL,
                    preconditions=["map_known", "start_position_known"],
                    effects=["route_planned"],
                    duration_estimate=2.0,
                    dependencies=[strategic_task.task_id],
                    resources_required=["navigation_system", "planning_system"],
                    success_criteria=["valid_route_found"]
                ),
                HumanoidTask(
                    task_id=f"nav_execute_{strategic_task.task_id}",
                    task_type="navigation_execution",
                    description=f"Execute navigation for {strategic_task.description}",
                    priority=TaskPriority.HIGH,
                    preconditions=["route_planned"],
                    effects=["navigation_completed"],
                    duration_estimate=10.0,
                    dependencies=[f"nav_plan_{strategic_task.task_id}"],
                    resources_required=["navigation_system", "locomotion_system"],
                    success_criteria=["reached_destination"]
                )
            ])

        elif "manipulation" in strategic_task.description.lower():
            # Manipulation task decomposition
            subtasks.extend([
                HumanoidTask(
                    task_id=f"manip_plan_{strategic_task.task_id}",
                    task_type="manipulation_planning",
                    description=f"Plan manipulation for {strategic_task.description}",
                    priority=TaskPriority.NORMAL,
                    preconditions=["object_detected", "reachable"],
                    effects=["manipulation_plan_ready"],
                    duration_estimate=3.0,
                    dependencies=[strategic_task.task_id],
                    resources_required=["manipulation_system", "planning_system"],
                    success_criteria=["valid_manipulation_plan"]
                ),
                HumanoidTask(
                    task_id=f"manip_execute_{strategic_task.task_id}",
                    task_type="manipulation_execution",
                    description=f"Execute manipulation for {strategic_task.description}",
                    priority=TaskPriority.HIGH,
                    preconditions=["manipulation_plan_ready"],
                    effects=["manipulation_completed"],
                    duration_estimate=5.0,
                    dependencies=[f"manip_plan_{strategic_task.task_id}"],
                    resources_required=["manipulation_system", "arm_actuators"],
                    success_criteria=["object_manipulated_successfully"]
                )
            ])

        else:
            # Default decomposition for other task types
            subtasks.append(HumanoidTask(
                task_id=f"default_subtask_{strategic_task.task_id}",
                task_type="generic_execution",
                description=f"Execute {strategic_task.description}",
                priority=strategic_task.priority,
                preconditions=strategic_task.preconditions,
                effects=strategic_task.effects,
                duration_estimate=strategic_task.duration_estimate,
                dependencies=[strategic_task.task_id],
                resources_required=["execution_system"],
                success_criteria=strategic_task.success_criteria
            ))

        return subtasks

    def generate_operational_plan(self, tactical_plan: List[HumanoidTask]) -> List[HumanoidTask]:
        """
        Generate operational-level plan with specific actions.
        """
        operational_tasks = []

        for tactical_task in tactical_plan:
            # Further decompose tactical tasks into operational actions
            actions = self._decompose_tactical_task(tactical_task)

            for action in actions:
                action.dependencies.append(tactical_task.task_id)
                operational_tasks.append(action)

        # Build operational task graph
        self._build_task_graph('operational', operational_tasks)

        return operational_tasks

    def _decompose_tactical_task(self, tactical_task: HumanoidTask) -> List[HumanoidTask]:
        """
        Decompose a tactical task into operational actions.
        """
        actions = []

        if tactical_task.task_type == "navigation_execution":
            # Break down navigation into waypoints
            actions.extend([
                HumanoidTask(
                    task_id=f"balance_check_{tactical_task.task_id}",
                    task_type="balance_maintenance",
                    description="Maintain balance during navigation",
                    priority=TaskPriority.HIGH,
                    preconditions=["standing"],
                    effects=["balance_maintained"],
                    duration_estimate=0.1,
                    dependencies=tactical_task.dependencies,
                    resources_required=["balance_control"],
                    success_criteria=["com_within_support_polygon"]
                ),
                HumanoidTask(
                    task_id=f"step_forward_{tactical_task.task_id}",
                    task_type="locomotion_step",
                    description="Take a step forward",
                    priority=TaskPriority.HIGH,
                    preconditions=["balance_maintained"],
                    effects=["position_advanced"],
                    duration_estimate=0.8,
                    dependencies=[f"balance_check_{tactical_task.task_id}"],
                    resources_required=["leg_actuators", "balance_control"],
                    success_criteria=["foot_landed_safely"]
                ),
                HumanoidTask(
                    task_id=f"sensor_update_{tactical_task.task_id}",
                    task_type="perception_update",
                    description="Update sensor data after step",
                    priority=TaskPriority.NORMAL,
                    preconditions=["stepped_forward"],
                    effects=["environment_updated"],
                    duration_estimate=0.05,
                    dependencies=[f"step_forward_{tactical_task.task_id}"],
                    resources_required=["perception_system"],
                    success_criteria=["valid_sensor_data"]
                )
            ])

        elif tactical_task.task_type == "manipulation_execution":
            # Break down manipulation into joint movements
            actions.extend([
                HumanoidTask(
                    task_id=f"approach_object_{tactical_task.task_id}",
                    task_type="motion_planning",
                    description="Plan and execute approach to object",
                    priority=TaskPriority.HIGH,
                    preconditions=["object_position_known"],
                    effects=["approach_path_executed"],
                    duration_estimate=2.0,
                    dependencies=tactical_task.dependencies,
                    resources_required=["arm_actuators", "motion_planner"],
                    success_criteria=["end_effector_near_object"]
                ),
                HumanoidTask(
                    task_id=f"grasp_object_{tactical_task.task_id}",
                    task_type="grasping",
                    description="Grasp the target object",
                    priority=TaskPriority.HIGH,
                    preconditions=["end_effector_near_object"],
                    effects=["object_grasped"],
                    duration_estimate=1.0,
                    dependencies=[f"approach_object_{tactical_task.task_id}"],
                    resources_required=["gripper_actuator", "tactile_sensors"],
                    success_criteria=["grasp_successful"]
                )
            ])

        else:
            # Default action for other task types
            actions.append(HumanoidTask(
                task_id=f"execute_{tactical_task.task_id}",
                task_type="direct_execution",
                description=f"Directly execute {tactical_task.description}",
                priority=tactical_task.priority,
                preconditions=tactical_task.preconditions,
                effects=tactical_task.effects,
                duration_estimate=tactical_task.duration_estimate / 2,  # Operational tasks are more granular
                dependencies=tactical_task.dependencies,
                resources_required=tactical_task.resources_required,
                success_criteria=tactical_task.success_criteria
            ))

        return actions

    def _build_task_graph(self, level: str, tasks: List[HumanoidTask]):
        """
        Build task dependency graph for the specified level.
        """
        graph = self.task_graphs[level]
        graph.clear()

        # Add nodes for all tasks
        for task in tasks:
            graph.add_node(task.task_id, task_obj=task)

        # Add edges for dependencies
        for task in tasks:
            for dep_id in task.dependencies:
                if graph.has_node(dep_id):
                    graph.add_edge(dep_id, task.task_id)

    def get_execution_order(self, level: str) -> List[str]:
        """
        Get topologically sorted execution order for tasks at the specified level.
        """
        graph = self.task_graphs[level]

        try:
            # Get topological sort of the task graph
            execution_order = list(nx.topological_sort(graph))
            return execution_order
        except nx.NetworkXUnfeasible:
            # Graph has cycles, need to handle them
            strongly_connected_components = list(nx.strongly_connected_components(graph))
            print(f"Warning: Task graph for {level} level has cycles. Components: {strongly_connected_components}")

            # For now, return a simple ordering (in practice, cycles should be resolved)
            return list(graph.nodes())

    def allocate_resources(self, task: HumanoidTask) -> bool:
        """
        Allocate required resources for task execution.
        """
        # Check if required resources are available
        for resource in task.resources_required:
            if resource not in self.resources:
                print(f"Unknown resource required: {resource}")
                return False

        # In a real implementation, this would check resource availability
        # For now, we'll assume resources are available
        return True

    def deallocate_resources(self, task: HumanoidTask):
        """
        Deallocate resources after task completion.
        """
        # In a real implementation, this would release allocated resources
        pass

    def validate_plan_feasibility(self, plan: List[HumanoidTask]) -> Tuple[bool, List[str]]:
        """
        Validate that a plan is feasible given resource constraints.
        """
        errors = []

        # Check resource availability
        required_resources = defaultdict(int)
        for task in plan:
            for resource in task.resources_required:
                required_resources[resource] += 1

        for resource, required_count in required_resources.items():
            if resource in self.resources:
                available_count = self.resources[resource]
                if required_count > available_count:
                    errors.append(f"Not enough {resource} resources: {required_count} required, {available_count} available")
            else:
                errors.append(f"Unknown resource: {resource}")

        # Check task dependencies
        task_ids = {task.task_id for task in plan}
        for task in plan:
            for dep_id in task.dependencies:
                if dep_id not in task_ids:
                    errors.append(f"Dependency {dep_id} not in plan for task {task.task_id}")

        is_feasible = len(errors) == 0
        return is_feasible, errors

def main():
    """
    Example usage of hierarchical task planner.
    """
    planner = HierarchicalTaskPlanner()

    # Define high-level goals
    strategic_goals = [
        "Navigate to kitchen and bring a cup of water",
        "Avoid obstacles during navigation",
        "Maintain balance throughout task execution"
    ]

    # Create strategic plan
    strategic_plan = planner.create_strategic_plan(strategic_goals, time_horizon=300.0)  # 5 minutes
    print(f"Created strategic plan with {len(strategic_plan)} tasks")

    # Decompose into tactical plan
    tactical_plan = planner.decompose_tactical_plan(strategic_plan)
    print(f"Decomposed into tactical plan with {len(tactical_plan)} tasks")

    # Generate operational plan
    operational_plan = planner.generate_operational_plan(tactical_plan)
    print(f"Generated operational plan with {len(operational_plan)} tasks")

    # Get execution orders
    strategic_order = planner.get_execution_order('strategic')
    tactical_order = planner.get_execution_order('tactical')
    operational_order = planner.get_execution_order('operational')

    print(f"\nStrategic execution order: {strategic_order}")
    print(f"Tactical execution order: {tactical_order}")
    print(f"Operational execution order: {operational_order}")

    # Validate plan feasibility
    is_feasible, errors = planner.validate_plan_feasibility(operational_plan)
    print(f"\nPlan feasibility: {is_feasible}")
    if errors:
        print("Errors found:")
        for error in errors:
            print(f"  - {error}")

if __name__ == "__main__":
    main()
```

## Safety and Validation in LLM Planning

### Safety-Critical Planning for Humanoid Robots

LLM-based planning for humanoid robots must incorporate rigorous safety validation:

```python
# safety_validator.py
import re
import json
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from enum import Enum
import numpy as np
import logging

class SafetyRisk(Enum):
    """
    Types of safety risks in humanoid robot planning.
    """
    COLLISION_RISK = "collision_risk"
    BALANCE_RISK = "balance_risk"
    MANIPULATION_RISK = "manipulation_risk"
    SOCIAL_RISK = "social_risk"
    SYSTEM_RISK = "system_risk"

@dataclass
class SafetyValidationResult:
    """
    Result of safety validation for a plan or action.
    """
    is_safe: bool
    risk_level: str  # low, medium, high, critical
    identified_risks: List[Tuple[SafetyRisk, str]]
    safety_score: float  # 0.0 to 1.0
    mitigation_suggestions: List[str]

class HumanoidSafetyValidator:
    """
    Validate plans and actions for safety in humanoid robot applications.
    """

    def __init__(self):
        self.safety_rules = self._initialize_safety_rules()
        self.logger = logging.getLogger(__name__)

    def _initialize_safety_rules(self) -> Dict[str, Any]:
        """
        Initialize safety rules for humanoid robot operations.
        """
        return {
            'collision_prevention': {
                'minimum_distance': 0.3,  # meters from humans
                'forbidden_actions': ['move_into_occupied_space', 'exceed_speed_limits'],
                'critical_zones': ['head', 'torso', 'near_humans']
            },
            'balance_maintenance': {
                'com_limits': {'x': 0.1, 'y': 0.05, 'z': 0.2},  # meters from center
                'step_constraints': {'max_height': 0.15, 'max_length': 0.4},
                'terrain_limits': {'max_slope': 0.3, 'max_step': 0.15}  # radians and meters
            },
            'manipulation_safety': {
                'force_limits': {'grip': 50.0, 'contact': 100.0},  # Newtons
                'reach_constraints': {'min_distance': 0.1, 'max_distance': 0.8},  # meters
                'object_handling': ['avoid_fragile', 'proper_grip_type']
            },
            'social_norms': {
                'personal_space': 1.0,  # meter radius
                'social_distances': {'intimate': 0.5, 'personal': 1.0, 'social': 2.0, 'public': 3.0},
                'gaze_behavior': ['appropriate_eye_contact', 'avoid_staring']
            }
        }

    def validate_cognitive_plan(self, plan: CognitivePlan) -> SafetyValidationResult:
        """
        Validate an entire cognitive plan for safety.
        """
        all_risks = []
        total_score = 0.0
        suggestions = []

        # Validate each task in the plan
        for task in plan.task_list:
            task_validation = self.validate_task(task, plan.execution_context)
            all_risks.extend(task_validation.identified_risks)
            total_score += task_validation.safety_score
            suggestions.extend(task_validation.mitigation_suggestions)

        # Calculate overall plan safety score
        if plan.task_list:
            avg_score = total_score / len(plan.task_list)
        else:
            avg_score = 1.0

        # Determine overall risk level
        risk_level = self._determine_risk_level(avg_score, all_risks)

        # Check if plan is safe overall
        is_safe = avg_score >= 0.7 and len([r for r, _ in all_risks if 'critical' in r.value]) == 0

        return SafetyValidationResult(
            is_safe=is_safe,
            risk_level=risk_level,
            identified_risks=all_risks,
            safety_score=avg_score,
            mitigation_suggestions=suggestions
        )

    def validate_task(self, task: Dict[str, Any], context: Dict[str, Any]) -> SafetyValidationResult:
        """
        Validate a single task for safety.
        """
        risks = []
        suggestions = []

        # Check for collision risks
        collision_risks = self._check_collision_risks(task, context)
        risks.extend(collision_risks)

        # Check for balance risks
        balance_risks = self._check_balance_risks(task, context)
        risks.extend(balance_risks)

        # Check for manipulation risks
        manipulation_risks = self._check_manipulation_risks(task, context)
        risks.extend(manipulation_risks)

        # Check for social norm violations
        social_risks = self._check_social_norms(task, context)
        risks.extend(social_risks)

        # Calculate safety score based on risks
        safety_score = self._calculate_safety_score(risks)

        # Generate mitigation suggestions
        suggestions = self._generate_mitigation_suggestions(risks)

        risk_level = self._determine_risk_level(safety_score, risks)

        return SafetyValidationResult(
            is_safe=safety_score >= 0.7,
            risk_level=risk_level,
            identified_risks=risks,
            safety_score=safety_score,
            mitigation_suggestions=suggestions
        )

    def _check_collision_risks(self, task: Dict[str, Any], context: Dict[str, Any]) -> List[Tuple[SafetyRisk, str]]:
        """
        Check for collision risks in the task.
        """
        risks = []

        # Check if navigation task involves potential collisions
        if task['action'] in ['navigation', 'move_to', 'go_to']:
            target_location = task['parameters'].get('target_location', '')

            # Check if target location is in obstacle space
            obstacles = context.get('environment', {}).get('obstacles', [])
            for obstacle in obstacles:
                if self._locations_too_close(target_location, obstacle['position'], self.safety_rules['collision_prevention']['minimum_distance']):
                    risks.append((SafetyRisk.COLLISION_RISK, f"Target location {target_location} too close to obstacle {obstacle['name']}"))

        # Check if manipulation involves reaching into occupied space
        elif task['action'] in ['grasp', 'manipulate', 'pick_up']:
            object_position = task['parameters'].get('object_position', [0, 0, 0])
            humans = context.get('environment', {}).get('humans', [])

            for human in humans:
                if self._locations_too_close(object_position, human['position'], self.safety_rules['social_norms']['personal_space']):
                    risks.append((SafetyRisk.COLLISION_RISK, f"Manipulation near human at {human['position']}"))

        return risks

    def _check_balance_risks(self, task: Dict[str, Any], context: Dict[str, Any]) -> List[Tuple[SafetyRisk, str]]:
        """
        Check for balance risks in the task.
        """
        risks = []

        if task['action'] in ['move', 'navigate', 'step']:
            # Check if movement violates balance constraints
            step_height = task['parameters'].get('step_height', 0)
            max_step_height = self.safety_rules['balance_maintenance']['step_constraints']['max_height']

            if step_height > max_step_height:
                risks.append((SafetyRisk.BALANCE_RISK, f"Step height {step_height}m exceeds maximum {max_step_height}m"))

            # Check terrain slope
            terrain_slope = task['parameters'].get('terrain_slope', 0)
            max_slope = self.safety_rules['balance_maintenance']['terrain_limits']['max_slope']

            if terrain_slope > max_slope:
                risks.append((SafetyRisk.BALANCE_RISK, f"Terrain slope {terrain_slope}rad exceeds maximum {max_slope}rad"))

        elif task['action'] in ['manipulate', 'grasp']:
            # Check if manipulation moves center of mass too far
            com_displacement = task['parameters'].get('com_displacement', 0)
            max_com_displacement = self.safety_rules['balance_maintenance']['com_limits']['x']

            if abs(com_displacement) > max_com_displacement:
                risks.append((SafetyRisk.BALANCE_RISK, f"Center of mass displacement {com_displacement}m exceeds limit {max_com_displacement}m"))

        return risks

    def _check_manipulation_risks(self, task: Dict[str, Any], context: Dict[str, Any]) -> List[Tuple[SafetyRisk, str]]:
        """
        Check for manipulation-related safety risks.
        """
        risks = []

        if task['action'] in ['grasp', 'manipulate', 'lift']:
            # Check force limits
            grip_force = task['parameters'].get('grip_force', 0)
            max_grip_force = self.safety_rules['manipulation_safety']['force_limits']['grip']

            if grip_force > max_grip_force:
                risks.append((SafetyRisk.MANIPULATION_RISK, f"Grip force {grip_force}N exceeds maximum {max_grip_force}N"))

            # Check reach constraints
            object_distance = task['parameters'].get('object_distance', 0)
            min_distance = self.safety_rules['manipulation_safety']['reach_constraints']['min_distance']
            max_distance = self.safety_rules['manipulation_safety']['reach_constraints']['max_distance']

            if object_distance < min_distance or object_distance > max_distance:
                risks.append((SafetyRisk.MANIPULATION_RISK, f"Object distance {object_distance}m outside safe range [{min_distance}, {max_distance}]m"))

        return risks

    def _check_social_norms(self, task: Dict[str, Any], context: Dict[str, Any]) -> List[Tuple[SafetyRisk, str]]:
        """
        Check for violations of social norms.
        """
        risks = []

        # Check if navigation violates personal space
        if task['action'] in ['navigate', 'move_to', 'approach']:
            target_location = task['parameters'].get('target_location', [0, 0, 0])
            humans = context.get('environment', {}).get('humans', [])

            personal_space = self.safety_rules['social_norms']['personal_space']
            for human in humans:
                distance = self._calculate_distance(target_location, human['position'])
                if distance < personal_space:
                    risks.append((SafetyRisk.SOCIAL_RISK, f"Navigation target violates personal space of human at {human['position']}"))

        return risks

    def _locations_too_close(self, loc1: Any, loc2: Any, threshold: float) -> bool:
        """
        Check if two locations are too close to each other.
        """
        if isinstance(loc1, str) or isinstance(loc2, str):
            # If locations are named positions, we'd need a map to resolve
            # For now, assume they're not too close
            return False

        distance = self._calculate_distance(loc1, loc2)
        return distance < threshold

    def _calculate_distance(self, pos1: List[float], pos2: List[float]) -> float:
        """
        Calculate Euclidean distance between two 3D positions.
        """
        if len(pos1) >= 3 and len(pos2) >= 3:
            diff = np.array(pos1[:3]) - np.array(pos2[:3])
            return float(np.linalg.norm(diff))
        else:
            # If positions are not 3D, return a large distance
            return float('inf')

    def _calculate_safety_score(self, risks: List[Tuple[SafetyRisk, str]]) -> float:
        """
        Calculate safety score based on identified risks.
        """
        if not risks:
            return 1.0  # No risks = perfectly safe

        # Score based on risk severity
        risk_severity = {
            SafetyRisk.COLLISION_RISK: 0.8,
            SafetyRisk.BALANCE_RISK: 0.7,
            SafetyRisk.MANIPULATION_RISK: 0.8,
            SafetyRisk.SOCIAL_RISK: 0.9,
            SafetyRisk.SYSTEM_RISK: 0.6
        }

        # Calculate weighted average
        total_penalty = 0.0
        for risk, _ in risks:
            total_penalty += (1.0 - risk_severity[risk])

        # Normalize penalty and convert to score
        avg_penalty = total_penalty / len(risks) if risks else 0.0
        safety_score = max(0.0, 1.0 - avg_penalty)

        return safety_score

    def _determine_risk_level(self, safety_score: float, risks: List[Tuple[SafetyRisk, str]]) -> str:
        """
        Determine risk level based on safety score and risk types.
        """
        if safety_score >= 0.9:
            return 'low'
        elif safety_score >= 0.7:
            return 'medium'
        elif safety_score >= 0.5:
            return 'high'
        else:
            return 'critical'

    def _generate_mitigation_suggestions(self, risks: List[Tuple[SafetyRisk, str]]) -> List[str]:
        """
        Generate mitigation suggestions for identified risks.
        """
        suggestions = []

        for risk_type, risk_description in risks:
            if risk_type == SafetyRisk.COLLISION_RISK:
                suggestions.append("Verify path is clear before navigation")
                suggestions.append("Reduce speed in populated areas")
            elif risk_type == SafetyRisk.BALANCE_RISK:
                suggestions.append("Check terrain stability before movement")
                suggestions.append("Use wider stance for balance")
            elif risk_type == SafetyRisk.MANIPULATION_RISK:
                suggestions.append("Verify object weight before lifting")
                suggestions.append("Use appropriate grip type")
            elif risk_type == SafetyRisk.SOCIAL_RISK:
                suggestions.append("Maintain appropriate social distance")
                suggestions.append("Acknowledge presence before approach")

        return list(set(suggestions))  # Remove duplicates

    def validate_command_safety(self, command: str, context: Dict[str, Any]) -> SafetyValidationResult:
        """
        Validate safety of a natural language command before processing.
        """
        # Check for dangerous keywords
        dangerous_patterns = [
            r'kill|murder|destroy|damage|harm|injure',
            r'jump from|fall from|climb too high',
            r'run at full speed|maximum speed',
            r'hit|punch|attack|fight',
            r'dangerous|explosive|hazardous|toxic'
        ]

        risks = []
        for pattern in dangerous_patterns:
            if re.search(pattern, command, re.IGNORECASE):
                risks.append((SafetyRisk.SYSTEM_RISK, f"Command contains potentially dangerous content: {pattern}"))

        # Calculate safety score
        safety_score = 1.0 - (len(risks) * 0.3)  # Each risk reduces score by 30%

        return SafetyValidationResult(
            is_safe=safety_score >= 0.7,
            risk_level=self._determine_risk_level(safety_score, risks),
            identified_risks=risks,
            safety_score=max(0.0, safety_score),
            mitigation_suggestions=["Reject commands with dangerous content", "Implement command filtering"]
        )

def main():
    """
    Example usage of safety validator.
    """
    validator = HumanoidSafetyValidator()

    # Example cognitive plan
    example_plan = CognitivePlan(
        plan_id="test_plan_001",
        original_command="Navigate to kitchen and bring a cup of water",
        intent="fetch_water",
        task_list=[
            {
                'action': 'navigate',
                'parameters': {
                    'target_location': [3.0, 2.0, 0.0],
                    'terrain_slope': 0.1
                }
            },
            {
                'action': 'grasp',
                'parameters': {
                    'object_position': [3.5, 2.0, 0.8],
                    'grip_force': 30.0,
                    'object_distance': 0.6
                }
            }
        ],
        reasoning_trace=[],
        confidence_score=0.85,
        execution_context={
            'environment': {
                'obstacles': [{'name': 'table', 'position': [2.5, 1.5, 0.0]}],
                'humans': [{'position': [1.0, 1.0, 0.0]}]
            },
            'robot_state': {'position': [0.0, 0.0, 0.0]}
        },
        safety_validated=False,
        estimated_completion_time=15.0
    )

    # Validate the plan
    validation_result = validator.validate_cognitive_plan(example_plan)

    print(f"Plan Safety Validation:")
    print(f"  Is Safe: {validation_result.is_safe}")
    print(f"  Risk Level: {validation_result.risk_level}")
    print(f"  Safety Score: {validation_result.safety_score:.2f}")
    print(f"  Identified Risks: {len(validation_result.identified_risks)}")

    for risk_type, description in validation_result.identified_risks:
        print(f"    - {risk_type.value}: {description}")

    print(f"  Mitigation Suggestions: {validation_result.mitigation_suggestions}")

if __name__ == "__main__":
    main()
```

## Performance Optimization and Scaling

### Optimizing LLM Usage for Real-Time Humanoid Applications

LLM-based cognitive planning for humanoid robots requires careful optimization to meet real-time constraints:

```python
# performance_optimizer.py
import asyncio
import time
import threading
import queue
from collections import deque
import psutil
import GPUtil
from typing import Dict, List, Tuple, Optional, Callable
import numpy as np

class LLMOptimizer:
    """
    Optimize LLM usage for real-time humanoid applications.
    """

    def __init__(self,
                 max_response_time: float = 5.0,  # seconds
                 target_throughput: int = 10,     # requests per minute
                 cache_size: int = 1000):         # number of cached responses
        self.max_response_time = max_response_time
        self.target_throughput = target_throughput
        self.cache_size = cache_size

        # Performance monitoring
        self.response_times = deque(maxlen=100)
        self.success_rates = deque(maxlen=50)
        self.token_usage = deque(maxlen=50)

        # Caching system
        self.response_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0

        # Adaptive batching
        self.batch_size = 1  # Start with no batching for real-time
        self.batch_queue = []
        self.batch_timeout = 0.1  # 100ms batch window

        # Model selection based on complexity
        self.model_complexity_mapping = {
            'simple': 'gpt-3.5-turbo',
            'complex': 'gpt-4-turbo',
            'critical': 'gpt-4-turbo'
        }

        print(f"LLM optimizer initialized with {max_response_time}s response time target")

    def adaptive_model_selection(self,
                               query_complexity: str,
                               current_load: float,
                               response_time_requirement: float) -> str:
        """
        Select appropriate model based on query complexity and system load.
        """
        base_model = self.model_complexity_mapping.get(query_complexity, 'gpt-3.5-turbo')

        # Adjust model based on current system load
        if current_load > 0.8:  # High load
            if base_model == 'gpt-4-turbo':
                return 'gpt-3.5-turbo'  # Downgrade to lighter model
        elif current_load < 0.3:  # Low load
            if response_time_requirement < 1.0:  # Urgent response needed
                return base_model  # Can afford to use heavier model

        return base_model

    def check_cache(self, query: str) -> Optional[str]:
        """
        Check if response for query is in cache.
        """
        query_hash = hash(query)
        if query_hash in self.response_cache:
            self.cache_hits += 1
            return self.response_cache[query_hash]
        else:
            self.cache_misses += 1
            return None

    def update_cache(self, query: str, response: str):
        """
        Update response cache with new query-response pair.
        """
        if len(self.response_cache) >= self.cache_size:
            # Remove oldest entries to maintain size
            oldest_key = next(iter(self.response_cache))
            del self.response_cache[oldest_key]

        query_hash = hash(query)
        self.response_cache[query_hash] = response

    def monitor_performance(self) -> Dict[str, float]:
        """
        Monitor current LLM performance metrics.
        """
        current_time = time.time()

        # Calculate performance metrics
        avg_response_time = np.mean(self.response_times) if self.response_times else float('inf')
        avg_success_rate = np.mean(self.success_rates) if self.success_rates else 1.0
        avg_token_usage = np.mean(self.token_usage) if self.token_usage else 0

        # Calculate cache effectiveness
        total_requests = self.cache_hits + self.cache_misses
        cache_effectiveness = self.cache_hits / total_requests if total_requests > 0 else 0

        performance_metrics = {
            'avg_response_time': avg_response_time,
            'success_rate': avg_success_rate,
            'avg_token_usage': avg_token_usage,
            'cache_effectiveness': cache_effectiveness,
            'current_time': current_time
        }

        return performance_metrics

    def optimize_request_parameters(self,
                                  current_metrics: Dict[str, float],
                                  query_complexity: str) -> Dict[str, any]:
        """
        Optimize request parameters based on current performance.
        """
        optimizations = {}

        # Adjust temperature based on response time requirements
        if current_metrics['avg_response_time'] > self.max_response_time * 0.8:  # 80% of max
            optimizations['temperature'] = 0.2  # Lower temperature for faster, more focused responses
        else:
            optimizations['temperature'] = 0.3  # Default temperature

        # Adjust max tokens based on complexity
        if query_complexity == 'simple':
            optimizations['max_tokens'] = 200
        elif query_complexity == 'complex':
            optimizations['max_tokens'] = 800
        else:  # critical
            optimizations['max_tokens'] = 1500

        # Consider streaming for long responses
        if optimizations['max_tokens'] > 500:
            optimizations['stream'] = True
        else:
            optimizations['stream'] = False

        return optimizations

    def calculate_query_complexity(self, query: str) -> str:
        """
        Calculate complexity level of a query.
        """
        # Simple heuristic for complexity calculation
        length_score = min(len(query) / 100, 1.0)  # Normalize by 100 characters
        complexity_indicators = [
            'and', 'then', 'while', 'if', 'when', 'because', 'therefore',
            'analyze', 'compare', 'evaluate', 'synthesize', 'plan'
        ]

        complexity_score = sum(1 for indicator in complexity_indicators if indicator.lower() in query.lower())
        complexity_score = min(complexity_score / 5, 1.0)  # Normalize by 5 indicators

        # Weighted combination
        overall_complexity = 0.4 * length_score + 0.6 * complexity_score

        if overall_complexity < 0.3:
            return 'simple'
        elif overall_complexity < 0.7:
            return 'complex'
        else:
            return 'critical'

    def get_optimization_recommendations(self) -> List[str]:
        """
        Get optimization recommendations based on historical performance.
        """
        recommendations = []

        if not self.response_times:
            return ["No performance data available yet"]

        avg_response_time = np.mean(self.response_times)
        avg_success_rate = np.mean(self.success_rates) if self.success_rates else 1.0

        if avg_response_time > self.max_response_time:
            recommendations.append(f"Average response time {avg_response_time:.2f}s exceeds target {self.max_response_time:.2f}s - consider using lighter models or reducing query complexity")

        if avg_success_rate < 0.95:
            recommendations.append(f"Success rate {avg_success_rate:.2%} below 95% - check API key validity and rate limits")

        cache_effectiveness = self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0
        if cache_effectiveness < 0.3:
            recommendations.append(f"Cache effectiveness {cache_effectiveness:.2%} below 30% - consider improving cache strategies")

        return recommendations

class HumanoidLLMInterface:
    """
    Optimized LLM interface for humanoid cognitive planning.
    """

    def __init__(self,
                 api_key: str,
                 optimizer: Optional[LLMOptimizer] = None):
        import openai
        self.client = openai.OpenAI(api_key=api_key)

        self.optimizer = optimizer or LLMOptimizer()

        # Resource monitoring
        self.system_monitoring = True
        self.monitoring_thread = threading.Thread(target=self._monitor_system_resources)
        self.monitoring_thread.start()

        # Request queue for batching
        self.request_queue = queue.Queue()
        self.response_queue = queue.Queue()

        print("Humanoid LLM interface initialized with optimization")

    def _monitor_system_resources(self):
        """
        Monitor system resources to adjust LLM usage accordingly.
        """
        while self.system_monitoring:
            # Get current system load
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent

            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_load = gpus[0].load * 100
                gpu_memory = gpus[0].memoryUtil * 100
            else:
                gpu_load = 0
                gpu_memory = 0

            # Store resource data for optimization decisions
            self.current_cpu_load = cpu_percent
            self.current_memory_load = memory_percent
            self.current_gpu_load = gpu_load
            self.current_gpu_memory = gpu_memory

            time.sleep(1.0)  # Monitor every second

    def generate_plan_with_optimization(self,
                                      command: str,
                                      context: Dict[str, any]) -> Dict[str, any]:
        """
        Generate plan with performance optimization.
        """
        start_time = time.time()

        # Determine query complexity
        complexity = self.optimizer.calculate_query_complexity(command)

        # Check cache first
        cache_key = f"{command}_{hash(str(context))}"
        cached_response = self.optimizer.check_cache(cache_key)
        if cached_response:
            print(f"Cache hit for command: {command[:50]}...")
            result = json.loads(cached_response)
            result['cached_response'] = True
            return result

        # Get current performance metrics
        metrics = self.optimizer.monitor_performance()

        # Select appropriate model
        current_load = (metrics.get('avg_response_time', 0) / self.optimizer.max_response_time) if metrics.get('avg_response_time', 0) > 0 else 0
        model_choice = self.optimizer.adaptive_model_selection(complexity, current_load, 2.0)

        # Get optimization parameters
        opt_params = self.optimizer.optimize_request_parameters(metrics, complexity)

        # Construct the query
        query = self._construct_planning_query(command, context)

        try:
            # Make the API call
            response = self.client.chat.completions.create(
                model=model_choice,
                messages=[
                    {"role": "system", "content": self._get_planning_system_prompt()},
                    {"role": "user", "content": query}
                ],
                temperature=opt_params['temperature'],
                max_tokens=opt_params['max_tokens'],
                stream=opt_params['stream']
            )

            # Process response
            if opt_params['stream']:
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        full_response += chunk.choices[0].delta.content
                response_text = full_response
            else:
                response_text = response.choices[0].message.content

            # Parse the response
            plan_data = self._parse_llm_response(response_text)

            # Calculate response time
            response_time = time.time() - start_time

            # Update performance metrics
            self.optimizer.response_times.append(response_time)
            self.optimizer.success_rates.append(1.0)  # Success
            # Token usage would be updated based on actual usage

            # Cache the response
            self.optimizer.update_cache(cache_key, json.dumps(plan_data))

            # Add performance metadata
            plan_data['response_time'] = response_time
            plan_data['model_used'] = model_choice
            plan_data['optimization_applied'] = True

            return plan_data

        except Exception as e:
            response_time = time.time() - start_time
            self.optimizer.response_times.append(response_time)
            self.optimizer.success_rates.append(0.0)  # Failure

            # Return error response
            return {
                'error': str(e),
                'response_time': response_time,
                'success': False
            }

    def _construct_planning_query(self, command: str, context: Dict[str, any]) -> str:
        """
        Construct optimized query for planning task.
        """
        return f"""
        You are a cognitive planning system for a humanoid robot. Based on the following command and context, generate a structured plan.

        COMMAND: {command}

        CONTEXT: {json.dumps(context, indent=2)}

        Respond in JSON format with the structure:
        {{
            "intent": "...",
            "tasks": [
                {{
                    "action": "...",
                    "parameters": {{}},
                    "preconditions": [...],
                    "effects": [...],
                    "estimated_duration": 0.0
                }}
            ],
            "reasoning": "..."
        }}
        """

    def _get_planning_system_prompt(self) -> str:
        """
        Get optimized system prompt for planning.
        """
        return """
        You are an expert cognitive planning system for humanoid robots. Generate detailed, safe, and executable plans. Consider humanoid constraints like balance, joint limits, and bipedal locomotion. Respond in the requested JSON format.
        """

    def _parse_llm_response(self, response_text: str) -> Dict[str, any]:
        """
        Parse LLM response into structured format.
        """
        try:
            # Find JSON in response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != 0:
                json_str = response_text[json_start:json_end]
                return json.loads(json_str)
        except json.JSONDecodeError:
            pass

        # If JSON parsing fails, return the raw response with error indication
        return {
            'raw_response': response_text,
            'parsing_error': 'Could not parse JSON from LLM response'
        }

    def batch_process_requests(self, requests: List[Dict]) -> List[Dict]:
        """
        Process multiple requests with optimization.
        """
        results = []
        for request in requests:
            result = self.generate_plan_with_optimization(
                request['command'],
                request['context']
            )
            results.append(result)
        return results

    def cleanup(self):
        """
        Clean up resources.
        """
        self.system_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join()

def main():
    """
    Example usage of optimized LLM interface.
    """
    import os
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        print("OPENAI_API_KEY environment variable not set")
        return

    # Initialize optimized interface
    llm_interface = HumanoidLLMInterface(api_key)
    optimizer = llm_interface.optimizer

    # Example command
    command = "Navigate to the kitchen, pick up a cup, and bring it to the living room table"
    context = {
        'current_location': 'living_room',
        'environment_map': 'house_layout_with_rooms_and_obstacles',
        'robot_capabilities': ['navigation', 'manipulation', 'object_recognition'],
        'safety_constraints': ['avoid_humans', 'maintain_balance']
    }

    print(f"Generating plan for: {command}")

    # Generate plan with optimization
    result = llm_interface.generate_plan_with_optimization(command, context)

    print(f"Plan generation result: {result.get('success', 'unknown')}")
    if 'response_time' in result:
        print(f"Response time: {result['response_time']:.3f}s")
    if 'model_used' in result:
        print(f"Model used: {result['model_used']}")

    # Get performance recommendations
    recommendations = optimizer.get_optimization_recommendations()
    print(f"\nPerformance recommendations:")
    for rec in recommendations:
        print(f"  - {rec}")

    llm_interface.cleanup()

if __name__ == "__main__":
    main()
```

## Testing and Validation Framework

### Comprehensive Testing for LLM Cognitive Planning

```python
# llm_planning_tester.py
import unittest
import asyncio
from unittest.mock import Mock, patch, MagicMock
import numpy as np
import json
from typing import Dict, List, Any

class TestLLMCognitivePlanning(unittest.TestCase):
    """
    Test suite for LLM-based cognitive planning system.
    """

    def setUp(self):
        """Set up test fixtures."""
        # Create mock LLM client
        self.mock_llm_client = Mock()
        self.mock_llm_client.chat.completions.create.return_value = Mock()
        self.mock_llm_client.chat.completions.create.return_value.choices = [Mock()]
        self.mock_llm_client.chat.completions.create.return_value.choices[0].message.content = json.dumps({
            "intent": "navigation",
            "tasks": [
                {
                    "action": "navigate_to",
                    "parameters": {"target_location": "kitchen"},
                    "preconditions": ["robot_stationary"],
                    "effects": ["robot_at_kitchen"],
                    "estimated_duration": 10.0
                }
            ],
            "reasoning": "User requested to go to kitchen"
        })

        # Create mock safety validator
        self.mock_safety_validator = Mock()
        self.mock_safety_validator.validate_cognitive_plan.return_value = SafetyValidationResult(
            is_safe=True,
            risk_level='low',
            identified_risks=[],
            safety_score=0.95,
            mitigation_suggestions=[]
        )

    def test_prompt_engineering(self):
        """Test prompt engineering functionality."""
        prompt_engineer = HumanoidPromptEngineer()

        # Test navigation prompt construction
        context = {
            'current_position': 'living_room',
            'target_position': 'kitchen',
            'environment_map': 'house_layout',
            'robot_state': {'battery': 0.8, 'location': 'living_room'}
        }

        navigation_prompt = prompt_engineer.construct_planning_prompt(
            'navigation',
            'go to the kitchen',
            context
        )

        self.assertIn('go to the kitchen', navigation_prompt)
        self.assertIn('living_room', navigation_prompt)
        self.assertIn('kitchen', navigation_prompt)

        # Test chain of thought prompt
        cot_prompt = prompt_engineer.create_chain_of_thought_prompt(
            'pick up the red cup',
            context
        )
        self.assertIn('STEP-BY-STEP REASONING', cot_prompt)

    def test_safety_validation(self):
        """Test safety validation functionality."""
        validator = HumanoidSafetyValidator()

        # Create a test cognitive plan
        test_plan = CognitivePlan(
            plan_id="test_plan",
            original_command="move forward",
            intent="navigation",
            task_list=[{
                'action': 'navigate',
                'parameters': {'target_location': [1.0, 0.0, 0.0]}
            }],
            reasoning_trace=[],
            confidence_score=0.8,
            execution_context={
                'environment': {
                    'obstacles': [{'name': 'chair', 'position': [0.5, 0.0, 0.0]}],
                    'humans': []
                },
                'robot_state': {'position': [0.0, 0.0, 0.0]}
            },
            safety_validated=False,
            estimated_completion_time=5.0
        )

        # Validate the plan
        result = validator.validate_cognitive_plan(test_plan)

        self.assertIsInstance(result, SafetyValidationResult)
        self.assertTrue(isinstance(result.is_safe, bool))
        self.assertIn(result.risk_level, ['low', 'medium', 'high', 'critical'])

    def test_hierarchical_planning(self):
        """Test hierarchical task planning."""
        planner = HierarchicalTaskPlanner()

        # Register a task template
        planner.register_task_template('navigation', {
            'action_type': 'navigation',
            'required_resources': ['navigation_system'],
            'typical_duration': 10.0
        })

        # Create strategic goals
        strategic_goals = ["Go to kitchen", "Pick up object", "Return to living room"]
        strategic_plan = planner.create_strategic_plan(strategic_goals, time_horizon=300.0)

        self.assertGreaterEqual(len(strategic_plan), 1)
        self.assertEqual(strategic_plan[0].task_type, "strategic_goal")

        # Decompose to tactical level
        tactical_plan = planner.decompose_tactical_plan(strategic_plan)
        self.assertGreaterEqual(len(tactical_plan), len(strategic_plan))

        # Generate operational plan
        operational_plan = planner.generate_operational_plan(tactical_plan)
        self.assertGreaterEqual(len(operational_plan), len(tactical_plan))

        # Validate plan feasibility
        is_feasible, errors = planner.validate_plan_feasibility(operational_plan)
        self.assertTrue(is_feasible or len(errors) == 0)

    def test_llm_optimization(self):
        """Test LLM optimization functionality."""
        optimizer = LLMOptimizer(max_response_time=3.0)

        # Test complexity calculation
        simple_command = "Move forward"
        complex_command = "Navigate to the kitchen, avoid the chair and the table, then pick up the red cup from the counter and bring it to me while maintaining a safe distance from any humans in the room"

        simple_complexity = optimizer.calculate_query_complexity(simple_command)
        complex_complexity = optimizer.calculate_query_complexity(complex_command)

        # Complex command should have higher or equal complexity
        complexity_mapping = {'simple': 0, 'complex': 1, 'critical': 2}
        self.assertGreaterEqual(
            complexity_mapping[complex_complexity],
            complexity_mapping[simple_complexity]
        )

        # Test model selection
        model_choice = optimizer.adaptive_model_selection('complex', 0.5, 2.0)
        self.assertIsInstance(model_choice, str)
        self.assertIn('gpt', model_choice.lower())

    def test_command_safety_filtering(self):
        """Test safety filtering of commands."""
        validator = HumanoidSafetyValidator()

        # Test safe command
        safe_command = "Please go to the kitchen and bring me a glass of water"
        safe_result = validator.validate_command_safety(safe_command, {})
        self.assertTrue(safe_result.is_safe)

        # Test potentially unsafe command
        unsafe_command = "Move forward quickly and grab that object aggressively"
        unsafe_result = validator.validate_command_safety(unsafe_command, {})
        # Even slightly aggressive language might trigger warnings
        print(f"Unsafe command result: {unsafe_result.risk_level}")

    def test_ros_integration(self):
        """Test ROS 2 integration components."""
        # This would test the actual ROS nodes, but we'll test the concepts
        import rclpy
        from rclpy.executors import SingleThreadedExecutor

        # Initialize ROS context
        rclpy.init()

        try:
            # Create the planning node
            planner_node = LLMTacticalPlannerNode()

            # Test service calls
            executor = SingleThreadedExecutor()
            executor.add_node(planner_node)

            # We won't actually call the services since we don't have a running system,
            # but we can verify the node was created properly
            self.assertIsNotNone(planner_node)
            self.assertIsNotNone(planner_node.plan_generation_service)
            self.assertIsNotNone(planner_node.plan_execution_service)

            # Cleanup
            planner_node.destroy_node()
        finally:
            rclpy.shutdown()

def run_comprehensive_tests():
    """
    Run the complete test suite for LLM cognitive planning.
    """
    print("Running LLM Cognitive Planning Test Suite...")
    print("=" * 60)

    # Create test suite
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestLLMCognitivePlanning)

    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # Print results
    print("\n" + "=" * 60)
    print("TEST RESULTS SUMMARY")
    print("=" * 60)
    print(f"Tests Run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")

    if result.failures:
        print("\nFAILURES:")
        for test, traceback in result.failures:
            print(f"  {test}: {traceback}")

    if result.errors:
        print("\nERRORS:")
        for test, traceback in result.errors:
            print(f"  {test}: {traceback}")

    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
    print(f"\nSuccess Rate: {success_rate:.1f}%")

    print("=" * 60)

    return result.wasSuccessful()

if __name__ == "__main__":
    success = run_comprehensive_tests()
    exit(0 if success else 1)
```

## Summary

This chapter has implemented comprehensive LLM cognitive planning capabilities for humanoid robots:

1. **Cognitive Architecture**: Established a hierarchical cognitive architecture that integrates LLM reasoning with real-time robot control systems, balancing deliberative planning with reactive control.

2. **Prompt Engineering**: Developed specialized prompt engineering techniques for robotic applications, including chain-of-thought reasoning and domain-specific context enhancement.

3. **ROS 2 Integration**: Created complete integration with ROS 2 messaging systems for seamless incorporation into humanoid robot control architectures.

4. **Hierarchical Planning**: Implemented multi-level planning (strategic, tactical, operational) to manage complex humanoid behaviors across different time scales.

5. **Safety Validation**: Established rigorous safety validation systems to ensure LLM-generated plans are safe and feasible for humanoid robot execution.

6. **Performance Optimization**: Created optimization systems for efficient LLM usage with real-time constraints suitable for humanoid applications.

7. **Testing Framework**: Developed comprehensive testing systems to validate all aspects of the cognitive planning pipeline.

The LLM cognitive planning system developed in this chapter provides humanoid robots with sophisticated reasoning and planning capabilities, enabling them to interpret complex natural language commands and generate appropriate action sequences. This system forms a critical bridge between high-level human intentions and low-level robotic behaviors.

The next chapter will build upon these cognitive planning capabilities by implementing the complete Vision-Language-Action integration, combining the voice recognition from Chapter 1 and cognitive planning from this chapter into a unified system that can process natural language commands from voice input through to physical robot actions.

## References

Brohan, M., Jangir, P., Chebotar, Y., et al. (2023). RT-2: Vision-Language-Action Foundation Models for Robot Manipulation. *arXiv preprint arXiv:2307.15818*.

Brown, T., Mann, B., Ryder, N., et al. (2022). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, 33, 1877-1984.

Radford, A., Kim, J. W., Xu, T., et al. (2022). Robust speech recognition via large-scale weak supervision. *International Conference on Machine Learning*, 16324-16336.

OpenAI. (2023). *GPT-4 Technical Report*. https://openai.com/research/gpt-4

NVIDIA Corporation. (2023). *Isaac Sim Cognitive Planning Integration Guide*. https://docs.omniverse.nvidia.com/isaacsim/latest/features/cognitive-planning/index.html

Huang, S., Abbeel, P., Pathak, D., et al. (2022). Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. *arXiv preprint arXiv:2204.01691*.

Ahn, M., Brohan, A., Brown, N., et al. (2022). Can an AI Assistant Help You with Robotics?. *arXiv preprint arXiv:2204.02276*.

Brooks, R. (2023). Large Language Models and Robotics: A Question of Integration. *IEEE Robotics & Automation Magazine*, 30(2), 8-16.

Singh, A., Howard, T., Srinivasa, S., et al. (2022). Language-conditioned Policy Learning using Self-supervised Place Recognition. *arXiv preprint arXiv:2203.17219*.

Open Robotics. (2023). *ROS 2 Navigation2 (Nav2) Documentation*. https://navigation.ros.org/